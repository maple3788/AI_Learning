{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return x / rms\n",
    "\n",
    "\n",
    "class SimpleMTP(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int, num_heads: int = 3, nhead: int = 2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.rmsnorm = RMSNorm()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.unembed.weight = self.embed.weight\n",
    "\n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Linear(d_model * 2, d_model)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.transformers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor, init_hidden: torch.Tensor = None):\n",
    "        B, T = token_ids.shape\n",
    "        device = token_ids.device\n",
    "\n",
    "        embeds = self.embed(token_ids)\n",
    "\n",
    "        if init_hidden is None:\n",
    "            h0_seq = embeds\n",
    "        else:\n",
    "            h0_seq = init_hidden\n",
    "\n",
    "        outputs = []\n",
    "        max_i = T - self.num_heads\n",
    "        for i in range(0, max_i):\n",
    "            h_prev = h0_seq[:, i, :]\n",
    "            logits_k = []\n",
    "\n",
    "            for k in range(self.num_heads):\n",
    "                future_pos = i + (k + 1)\n",
    "                tok_embed = embeds[:, future_pos, :]\n",
    "\n",
    "                h_norm = self.rmsnorm(h_prev)\n",
    "                e_norm = self.rmsnorm(tok_embed)\n",
    "\n",
    "                merged = torch.cat((h_norm, e_norm), dim=-1)\n",
    "\n",
    "                proj: torch.Tensor = self.projections[k](merged)\n",
    "\n",
    "                x = proj.unsqueeze(0)\n",
    "                x = self.transformers[k](x)\n",
    "                h_curr = x.squeeze(0)\n",
    "\n",
    "                logits = self.unembed(h_curr)\n",
    "                logits_k.append(logits)\n",
    "\n",
    "                h_prev = h_curr\n",
    "            logits_k = torch.stack(logits_k, dim=1)\n",
    "            outputs.append(logits_k)\n",
    "\n",
    "        # D -> num_heads V -> vocab_size\n",
    "        # stack along sequence axis -> (T-D, B, D, V) then permute -> (B, T-D, D, V)\n",
    "        out = torch.stack(outputs)\n",
    "        out = out.permute(1, 0, 2, 3).contiguous()\n",
    "        return out"
   ],
   "id": "a3cd14923ad15c1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size, seq_len, d_model, vocab_size = 1, 8, 8, 5000\n",
    "model = SimpleMTP(d_model=d_model, vocab_size=vocab_size)\n",
    "tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(tokens)\n",
    "\n",
    "logits = model(tokens)\n",
    "\n",
    "print(logits.shape)\n",
    "\n",
    "print(logits[0, 0, 0])\n",
    "\n",
    "# get all predictions at i=0 as token IDs\n",
    "pred_ids = logits[0, 0].argmax(dim=-1)\n",
    "print(pred_ids)"
   ],
   "id": "aa3376d16362b3bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size, seq_len, vocab_size = 1, 8, 5000\n",
    "targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "logits = model(tokens)\n",
    "B, L, D, V = logits.shape\n",
    "_, T = targets.shape\n",
    "\n",
    "loss = 0.0\n",
    "for i in range(L):\n",
    "    for k in range(D):\n",
    "        logit_ik = logits[:, i, k, :]\n",
    "        target_ik = targets[:, i + (k + 1)]\n",
    "        print(logit_ik.shape,target_ik.shape)\n",
    "        loss += F.cross_entropy(logit_ik, target_ik)\n",
    "loss = loss / (L * D)\n",
    "\n",
    "print(loss.item())"
   ],
   "id": "bf4bb4cc0cae7e33",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
