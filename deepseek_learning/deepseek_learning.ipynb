{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGING_FACE_KEY\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", torch_dtype=torch.float32, device_map=\"auto\")"
   ],
   "id": "a7e139bdfae3a88e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from transformers import pipeline\n",
    "# pipe = pipeline(\"summarization\", model=\"\")\n",
    "# response = pipe(\"test\")\n",
    "# print(response)"
   ],
   "id": "4c907dc54e1f23e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Coding MLA from scratch",
   "id": "2cb46d16bd02caa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initial Setup and Parameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RopelessMLA(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, kv_latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dh = d_model // n_heads  # dimension per head\n",
    "\n",
    "        self.W_q = nn.Linear(self.d_model, self.d_model, bias=False)  # Query Projection\n",
    "        self.W_dkv = nn.Linear(self.d_model, kv_latent_dim, bias=False)  # Compress into laten KV space\n",
    "        self.W_uk = nn.Linear(kv_latent_dim, self.d_model, bias=False)  # Decompress K\n",
    "        self.W_uv = nn.Linear(kv_latent_dim, self.d_model, bias=False)  # Decompress v\n",
    "        self.W_o = nn.Linear(self.d_model, self.d_model, bias=False)  # Final output projection\n",
    "\n",
    "        self.ln = nn.LayerNorm(kv_latent_dim)\n",
    "        self.register_buffer(\"absorbed_k\", None)  # Holds W_q @ W_uk\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache=None, past_length=0):\n",
    "        B, S, D = x.size()\n",
    "\n",
    "        if self.absorbed_k is None:\n",
    "            absorbed_k = torch.matmul(self.W_q.weight, self.W_uk.weight)\n",
    "            self.absorbed_k = absorbed_k.view(self.n_heads, self.dh, -1)\n",
    "\n",
    "        new_c_kv = self.ln(self.W_dkv(x))\n",
    "        if kv_cache is None:\n",
    "            c_kv = new_c_kv\n",
    "        else:\n",
    "            c_kv = torch.cat([kv_cache, new_c_kv], dim=1)\n",
    "\n",
    "        S_full = c_kv.size(1)\n",
    "\n",
    "        v_full = self.W_uv(c_kv)\n",
    "        v = v_full.view(B, S_full, self.n_heads, self.dh).transpose(1, 2)\n",
    "\n",
    "        q = x.view(B, S, self.n_heads, self.dh)\n",
    "\n",
    "        attn_scores = torch.zeros(B, self.n_heads, S, S_full, device=x.device)\n",
    "        for h in range(self.n_heads):\n",
    "            tmp = torch.matmul(q[:, :, h], self.absorbed_k[h])\n",
    "            attn_scores[:, h] = torch.bmm(tmp, c_kv.transpose(1, 2))  # BMM does parallel multiplication\n",
    "\n",
    "        attn_scores = attn_scores / (self.dh ** 0.5)\n",
    "        mask = torch.tril(torch.ones((S, S_full), device=x.device), diagonal=past_length)\n",
    "        attn_scores = attn_scores.masked_fill(mask.view(1, 1, S, S_full) == 0, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = attn_scores.softmax(dim=-1)\n",
    "\n",
    "        out_heads = []\n",
    "        for h in range(self.n_heads):\n",
    "            context_h = torch.matmul(attn_weights[:, h], v [:, h])\n",
    "            out_heads.append(context_h)\n",
    "        out = torch.cat(out_heads, dim=-1)\n",
    "\n",
    "        return self.W_o(out), c_kv"
   ],
   "id": "adb95853a24b5dcc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
