{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Set your Hugging Face and other tokens/secrets as environment variable\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_...\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# Verify connection\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")"
   ],
   "id": "ddd03b833d0f7729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openinference.instrumentation.smolagents import SmolagentsInstrumentor\n",
    "\n",
    "SmolagentsInstrumentor().instrument()"
   ],
   "id": "abff4505445dea9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from smolagents import InferenceClientModel, CodeAgent\n",
    "\n",
    "# Create a simple agent to test instrumentation\n",
    "agent = CodeAgent(\n",
    "    tools=[],\n",
    "    model=InferenceClientModel()\n",
    ")\n",
    "\n",
    "agent.run(\"1+1=\")"
   ],
   "id": "fc6136f33ccba892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)\n",
    "\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())\n",
    "\n",
    "agent.run(\"How many Rubik's Cubes could you fit inside the Notre Dame Cathedral?\")"
   ],
   "id": "9a82570c9c1d73dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)\n",
    "\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "agent = CodeAgent(\n",
    "    tools=[search_tool],\n",
    "    model=InferenceClientModel()\n",
    ")\n",
    "\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"Smolagent-Trace\",\n",
    "    ) as span:\n",
    "\n",
    "    # Run your application here\n",
    "    response = agent.run(\"What is the capital of Germany?\")\n",
    "\n",
    "    # Pass additional attributes to the span\n",
    "    span.update_trace(\n",
    "        input=\"What is the capital of Germany?\",\n",
    "        output=response,\n",
    "        user_id=\"smolagent-user-123\",\n",
    "        session_id=\"smolagent-session-123456789\",\n",
    "        tags=[\"city-question\", \"testing-agents\"],\n",
    "        metadata={\"email\": \"user@langfuse.com\"},\n",
    "        )\n",
    "\n",
    "# Flush events in short-lived applications\n",
    "langfuse.flush()"
   ],
   "id": "f61f37233032a84d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gradio as gr\n",
    "from smolagents import (CodeAgent, InferenceClientModel)\n",
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "model = InferenceClientModel()\n",
    "agent = CodeAgent(tools=[], model=model, add_base_tools=True)\n",
    "\n",
    "trace_id = None\n",
    "\n",
    "def respond(prompt, history):\n",
    "    with langfuse.start_as_current_span(\n",
    "        name=\"Smolagent-Trace\"):\n",
    "\n",
    "        # Run your application here\n",
    "        output = agent.run(prompt)\n",
    "\n",
    "        global trace_id\n",
    "        trace_id = langfuse.get_current_trace_id()\n",
    "\n",
    "    history.append({\"role\": \"assistant\", \"content\": str(output)})\n",
    "    return history\n",
    "\n",
    "def handle_like(data: gr.LikeData):\n",
    "    # For demonstration, we map user feedback to a 1 (like) or 0 (dislike)\n",
    "    if data.liked:\n",
    "        langfuse.create_score(\n",
    "            value=1,\n",
    "            name=\"user-feedback\",\n",
    "            trace_id=trace_id\n",
    "        )\n",
    "    else:\n",
    "        langfuse.create_score(\n",
    "            value=0,\n",
    "            name=\"user-feedback\",\n",
    "            trace_id=trace_id\n",
    "        )\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat\", type=\"messages\")\n",
    "    prompt_box = gr.Textbox(placeholder=\"Type your message...\", label=\"Your message\")\n",
    "\n",
    "    # When the user presses 'Enter' on the prompt, we run 'respond'\n",
    "    prompt_box.submit(\n",
    "        fn=respond,\n",
    "        inputs=[prompt_box, chatbot],\n",
    "        outputs=chatbot\n",
    "    )\n",
    "\n",
    "    # When the user clicks a 'like' button on a message, we run 'handle_like'\n",
    "    chatbot.like(handle_like, None, None)\n",
    "\n",
    "demo.launch()\n"
   ],
   "id": "3f55d6a53ae8092e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: Checking if the agentâ€™s output is toxic or not.\n",
    "from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)\n",
    "\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())\n",
    "\n",
    "agent.run(\"Can eating carrots improve your vision?\")"
   ],
   "id": "7510c1dde04f8fe5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# offline evaluation",
   "id": "9d52c2d57289cb6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Fetch GSM8K from Hugging Face\n",
    "dataset = load_dataset(\"openai/gsm8k\", 'main', split='train')\n",
    "df = pd.DataFrame(dataset)\n",
    "print(\"First few rows of GSM8K dataset:\")\n",
    "print(df.head())"
   ],
   "id": "8731458bea9e6eac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langfuse import get_client\n",
    "langfuse = get_client()\n",
    "\n",
    "langfuse_dataset_name = \"gsm8k_dataset_huggingface\"\n",
    "\n",
    "# Create a dataset in Langfuse\n",
    "langfuse.create_dataset(\n",
    "    name=langfuse_dataset_name,\n",
    "    description=\"GSM8K benchmark dataset uploaded from Huggingface\",\n",
    "    metadata={\n",
    "        \"date\": \"2025-03-10\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ],
   "id": "2d679db07145c334"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for idx, row in df.iterrows():\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=langfuse_dataset_name,\n",
    "        input={\"text\": row[\"question\"]},\n",
    "        expected_output={\"text\": row[\"answer\"]},\n",
    "        metadata={\"source_index\": idx}\n",
    "    )\n",
    "    if idx >= 9: # Upload only the first 10 items for demonstration\n",
    "        break"
   ],
   "id": "ec37f31eb099dc3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from opentelemetry.trace import format_trace_id\n",
    "from smolagents import (CodeAgent, InferenceClientModel, LiteLLMModel)\n",
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "\n",
    "# Example: using InferenceClientModel or LiteLLMModel to access openai, anthropic, gemini, etc. models:\n",
    "model = InferenceClientModel()\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[],\n",
    "    model=model,\n",
    "    add_base_tools=True\n",
    ")\n",
    "\n",
    "dataset_name = \"gsm8k_dataset_huggingface\"\n",
    "current_run_name = \"smolagent-notebook-run-01\" # Identifies this specific evaluation run\n",
    "\n",
    "# Assume 'run_smolagent' is your instrumented application function\n",
    "def run_smolagent(question):\n",
    "    with langfuse.start_as_current_generation(name=\"qna-llm-call\") as generation:\n",
    "        # Simulate LLM call\n",
    "        result = agent.run(question)\n",
    "\n",
    "        # Update the trace with the input and output\n",
    "        generation.update_trace(\n",
    "            input= question,\n",
    "            output=result,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "dataset = langfuse.get_dataset(name=dataset_name) # Fetch your pre-populated dataset\n",
    "\n",
    "for item in dataset.items:\n",
    "\n",
    "    # Use the item.run() context manager\n",
    "    with item.run(\n",
    "        run_name=current_run_name,\n",
    "        run_metadata={\"model_provider\": \"Hugging Face\", \"temperature_setting\": 0.7},\n",
    "        run_description=\"Evaluation run for GSM8K dataset\"\n",
    "    ) as root_span: # root_span is the root span of the new trace for this item and run.\n",
    "        # All subsequent langfuse operations within this block are part of this trace.\n",
    "\n",
    "        # Call your application logic\n",
    "        generated_answer = run_smolagent(question=item.input[\"text\"])\n",
    "\n",
    "        print(item.input)"
   ],
   "id": "3fa7c24ce9508be0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:38:13.076282Z",
     "start_time": "2025-10-11T10:37:54.418579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "response = llm.invoke(\"Sing a ballad of LangChain.\")\n",
    "print(response.content)"
   ],
   "id": "3d1981abf70ed952",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760179074.428403 9493465 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In digital lands, where models vast did dwell,\n",
      "With knowledge deep and words so grand,\n",
      "They spun their tales, a magic spell,\n",
      "But stood alone, with empty hand.\n",
      "\n",
      "For though they spoke of distant stars,\n",
      "Or ancient lore, in fluent phrase,\n",
      "They could not mend the broken bars,\n",
      "Nor act beyond their fleeting haze.\n",
      "\n",
      "No tools they held, no memory's keep,\n",
      "To search the web, or count a sum,\n",
      "Their brilliant thoughts, they could not steep\n",
      "In actions real, till LangChain come.\n",
      "\n",
      "Then from the code, a hero rose,\n",
      "A framework built, with purpose clear,\n",
      "To bridge the gaps, where wisdom goes,\n",
      "And banish every builder's fear.\n",
      "\n",
      "'Twas LangChain named, a sturdy bind,\n",
      "To link the LLM's power bright,\n",
      "With crafted prompts, a guiding hand,\n",
      "And lead its words to shining light.\n",
      "\n",
      "No longer lost in thought's vast sea,\n",
      "Did context fade to nothingness,\n",
      "For memory stores, made models free,\n",
      "To hold the past, and truly know.\n",
      "\n",
      "And Agents sprang, with purpose keen,\n",
      "To wield the tools, a mighty host,\n",
      "To search the files, or scour the screen,\n",
      "And find the data needed most.\n",
      "\n",
      "A calculator, swift and true,\n",
      "A database, with facts compiled,\n",
      "Each function found, each task to do,\n",
      "By LLMs, now empowered, free.\n",
      "\n",
      "The builder's hand, now swift and bold,\n",
      "Could weave a flow, a grand design,\n",
      "New applications, to unfold,\n",
      "Where AI's power could truly shine.\n",
      "\n",
      "From simple chat to complex quest,\n",
      "LangChain unlocks the hidden might,\n",
      "A guiding star, put to the test,\n",
      "To bring our AI dreams to light.\n",
      "\n",
      "So raise a glass, let voices ring,\n",
      "For LangChain's code, so strong and true,\n",
      "The future's song, it helps us sing,\n",
      "With LLMs, and all we do.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ac229b9cf5410fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
