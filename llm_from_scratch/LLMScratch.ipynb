{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reading in a short story as text sample into Python.\n",
    "# Step 1: Creating Tokens"
   ],
   "id": "287bbdf8106a18c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from IPython.core.pylabtools import figsize\n",
    "from matplotlib.lines import lineStyles\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "print(\"Total number of character:\", len(raw_text))"
   ],
   "id": "be01134c7fda702c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "result = re.split(r'(\\s|--|[^a-zA-Z-])', raw_text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result[:30])\n",
    "print(len(result))"
   ],
   "id": "55261b4e3d3a2d15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2:Creating Token IDs\n",
    "each unique taken is mapped to a unique integer called takenID"
   ],
   "id": "5d8f8a943c912a05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(result))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}"
   ],
   "id": "e135f67baf864a03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_vocab = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'(\\s|--|[^a-zA-Z-])', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[item] for item in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_vocab[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ],
   "id": "b27253169ed4c8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "          Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "4ae086144d8067ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ADDING SPECIAL CONTEXT TOKENS",
   "id": "1071b567510f862f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(list(set(result)))\n",
    "all_tokens.extend(['<|unk|>', '<|endoftext|>'])\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "len(vocab)\n",
    "print(list(vocab.items())[-5:])"
   ],
   "id": "c377964a31a9661c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV2(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_vocab = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'(\\s|--|[^a-zA-Z-])', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[item] for item in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_vocab[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ],
   "id": "2d520dd76b20cfd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for out-of-vocabulary words.\n",
    "Instead, GPT use type pair encoding"
   ],
   "id": "499f0ae033036e57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BYTE PAIR ENCODING (BPE)\n",
    "BPE is a subword tokenization algorithm"
   ],
   "id": "bef0b71c37d92a8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import importlib\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ],
   "id": "1d60311b8bd9fcde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "        \"of someunknownPlace.\")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ],
   "id": "a652b2054b3ff6cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ],
   "id": "82bb6484eb91057e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "integers = tokenizer.encode(\"f i n e\")\n",
    "print(integers)"
   ],
   "id": "563430ae4c806aeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CREATING INPUT-TARGET PAIRS",
   "id": "b60aa3530965a65a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ],
   "id": "4d6b53212b3152f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "enc_sample = enc_text[50:]",
   "id": "1db02906b8c9317b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"----->\", desired)"
   ],
   "id": "bed6800a4a9a011d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPLEMENTING A DATA LOADER",
   "id": "1b4118fe8bcedec1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:11:07.795987Z",
     "start_time": "2025-05-18T06:11:07.791590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tiktoken import Encoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer: Encoding, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ],
   "id": "6600fea0961ca302",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:11:04.477806Z",
     "start_time": "2025-05-18T06:11:04.474557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ],
   "id": "14e78d771e0534a1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ],
   "id": "2a85743a62b14d71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPORT TRAINED MODEL",
   "id": "78f251020d5d70d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gensim.downloader as gensim_api\n",
    "\n",
    "model = gensim_api.load(\"word2vec-google-news-300\")  # download the model and return as an object ready to use"
   ],
   "id": "fbb87a56c0b80e05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(model['computer'])\n",
    "word_vectors = model\n",
    "# Example of using most_similar\n",
    "print(word_vectors.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10))\n",
    "\n",
    "# Example of calculating similarity\n",
    "print(word_vectors.similarity('woman', 'man'))\n",
    "print(word_vectors.similarity('king', 'queen'))\n",
    "print(word_vectors.similarity('boy', 'girl'))\n",
    "print(word_vectors.similarity('uncle', 'aunt'))\n",
    "print(word_vectors.similarity('nephew', 'niece'))\n",
    "print(word_vectors.similarity('paper', 'water'))"
   ],
   "id": "5510027db3111640",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(embedding_layer.weight)\n"
   ],
   "id": "f00aa4fdbe82af89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(embedding_layer(torch.tensor([3])))",
   "id": "57297c044c60396b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(embedding_layer(input_ids))\n",
    "\n",
    "print(torch.nn.Embedding(4, 5).weight)"
   ],
   "id": "a842b5fcfe999545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# POSITIONAL EMBEDDING",
   "id": "60e2e4aa6b49dee8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ],
   "id": "ecc875173c044c28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, target = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"shape:\\n\", inputs.shape)\n"
   ],
   "id": "e9c3f9189b699058",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "token_embeddings: torch.Tensor = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ],
   "id": "a4d4756cb4d8727",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context_length = max_length\n",
    "context_size = 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_size, output_dim)\n",
    "\n",
    "pos_embeddings: torch.Tensor = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings)"
   ],
   "id": "905857f8c44c6e54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_embedding = token_embeddings + pos_embeddings\n",
    "print(input_embedding.shape)"
   ],
   "id": "3f03e04d48143027",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPLEMENTING A SIMPLIFIED ATTENTION MECHANISM",
   "id": "870ecea5b26f5bc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Your\n",
    "    [0.55, 0.87, 0.66],  # journey\n",
    "    [0.57, 0.85, 0.64],  # starts\n",
    "    [0.22, 0.58, 0.33],  # with\n",
    "    [0.77, 0.25, 0.10],  # one\n",
    "    [0.05, 0.80, 0.55]  # step\n",
    "])"
   ],
   "id": "4c826c1eaf32f404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(attn_weights_2_tmp)"
   ],
   "id": "82336baa9f2cbccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "source": "%%sql\n",
   "id": "b82e19cd050ee990",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "\n",
    "# this naive softmax implementation may encounter numerical instability issues, such as overflow (for very large values) and underflow (for very small values)\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(attn_weights_2_naive)"
   ],
   "id": "dd342ebafee674ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(attn_weights_2)\n",
    "print(attn_weights_2.sum())"
   ],
   "id": "ee2b1b0988b1db4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = inputs[1]\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, i_x in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * i_x\n",
    "\n",
    "print(context_vec_2)\n"
   ],
   "id": "2385ce436dd767b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "print(attn_scores)\n",
    "\n",
    "for i, i_x in enumerate(inputs):\n",
    "    for j, j_x in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(i_x, j_x)\n",
    "print(attn_scores)\n"
   ],
   "id": "64c9ac8b621cfada",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ],
   "id": "f86ec1a139430ee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ],
   "id": "fb37cac2a1b27f85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS",
   "id": "58858a73d41d8fd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Your\n",
    "    [0.55, 0.87, 0.66],  # journey\n",
    "    [0.57, 0.85, 0.64],  # starts\n",
    "    [0.22, 0.58, 0.33],  # with\n",
    "    [0.77, 0.25, 0.10],  # one\n",
    "    [0.05, 0.80, 0.55]  # step\n",
    "])"
   ],
   "id": "69683fc32379f790",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ],
   "id": "a42dfced28c51d44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_2 = inputs[1]\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ],
   "id": "894195d27e99056e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "queries = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(queries)\n",
    "print(keys)\n",
    "print(values)"
   ],
   "id": "df833dd876f32ee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_2 = queries[1]\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ],
   "id": "7e753bb128cd3964",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attn_scores = queries @ keys.T\n",
    "print(attn_scores)"
   ],
   "id": "d7388ffae3064e36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "d_k = keys.shape[-1]\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / d_k ** 0.5, dim=-1)\n",
    "print(attn_weights)"
   ],
   "id": "4040bb579a7c9f2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WHY DIVIDE BY SQRT (DIMENSION)",
   "id": "51039ab6621b526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# reason 1: For stability in learning\n",
    "\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "softmax_result = torch.softmax(tensor, dim=-1)\n",
    "print(softmax_result)"
   ],
   "id": "87add113d30a3850",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BUT WHY SQRT?",
   "id": "6ec586aea3a2cbed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reason 2: To make the variance of the dot product stable close to 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trails=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trails):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "        print(q, k)\n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scale_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scale_dot_product)\n",
    "\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "\n",
    "variance_before_scaling_5, variance_after_scaling_5 = compute_variance(5)\n",
    "variance_before_scaling_100, variance_after_scaling_100 = compute_variance(100)\n"
   ],
   "id": "bf807c7323a28454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# COMPUTE CONTEXT VALUE",
   "id": "ecde4b1107bc8805"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ],
   "id": "e368cc8f7407dff9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context_vec = attn_weights @ values\n",
    "print(context_vec)"
   ],
   "id": "21530e348e7f9e70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS",
   "id": "68a3b0660c4273df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ],
   "id": "197c6ebac516c0cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ],
   "id": "2dea265b0ac792ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ],
   "id": "aac44460ae133dda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Your\n",
    "    [0.55, 0.87, 0.66],  # journey\n",
    "    [0.57, 0.85, 0.64],  # starts\n",
    "    [0.22, 0.58, 0.33],  # with\n",
    "    [0.77, 0.25, 0.10],  # one\n",
    "    [0.05, 0.80, 0.55]  # step\n",
    "])\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ],
   "id": "60cea8f9ba95f49b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HIDING FUTURE WORDS WITH CAUSAL ATTENTION",
   "id": "28200117867fca8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(789)\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores: torch.Tensor = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / inputs.shape[-1] ** 0.5, dim=-1)\n",
    "print(attn_weights)\n"
   ],
   "id": "45a9f1d3e980c452",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context_length = inputs.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ],
   "id": "264b8bca3125f016",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ],
   "id": "a5a60645601b817d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ],
   "id": "c7ee411056d068aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "attn_weights = torch.softmax(masked / inputs.shape[-1] ** 0.5, dim=1)\n",
    "print(attn_weights)"
   ],
   "id": "30cf1290790910a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT",
   "id": "d54027bc3f133306"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dropout is a deep learning technique where randomly selected hidden layer unit s are ignored during training\n",
    "# This prevents overfitting and improves generalization performance\n",
    "# applied in 2 specific areas  one is after calculating attention scores and the other is after applying attention weights to value vectors\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(example)\n",
    "print(dropout(example))"
   ],
   "id": "bfb597d77a81724a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(attn_weights)\n",
    "print(dropout(attn_weights))"
   ],
   "id": "23a25eafa2a04c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS",
   "id": "b98b60751507d41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# self test not the lecture content\n",
    "class MyAttention_V3(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        context_length = queries.shape[0]\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        attn_scores_masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores_masked / queries.shape[-1] ** 0.5, dim=1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Your\n",
    "    [0.55, 0.87, 0.66],  # journey\n",
    "    [0.57, 0.85, 0.64],  # starts\n",
    "    [0.22, 0.58, 0.33],  # with\n",
    "    [0.77, 0.25, 0.10],  # one\n",
    "    [0.05, 0.80, 0.55]  # step\n",
    "])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "mav = MyAttention_V3(3, 2)\n",
    "print(mav(inputs))\n"
   ],
   "id": "7fbe47f5b3953c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ],
   "id": "d11ee5fc60a12d19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys: torch.Tensor = self.W_key(x)\n",
    "        queries: torch.Tensor = self.W_query(x)\n",
    "        values: torch.Tensor = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / num_tokens ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)"
   ],
   "id": "63bc91f1c9086f43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EXTENDING SINGLE HEAD ATTENTION TO MULTI-HEAD ATTENTION",
   "id": "83cd41f1ad233eea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "ca = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, 2)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)"
   ],
   "id": "3bd9996667e729a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS",
   "id": "eb980107c2ff132d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\",\n",
    "                             torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores: torch.Tensor = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec: torch.Tensor = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Your\n",
    "    [0.55, 0.87, 0.66],  # journey\n",
    "    [0.57, 0.85, 0.64],  # starts\n",
    "    [0.22, 0.58, 0.33],  # with\n",
    "    [0.77, 0.25, 0.10],  # one\n",
    "    [0.05, 0.80, 0.55]  # step\n",
    "])\n",
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n",
    "context_length = batch.shape[1]\n",
    "d_in = 3\n",
    "d_out = 4\n",
    "ca = MultiHeadAttention(d_in, d_out, context_length, 0.0, 2)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)"
   ],
   "id": "174cf3057c609d95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT",
   "id": "47f434fc751d89ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ],
   "id": "903fca50d37c79fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT ARCHITECTURE PART 1: DUMMY GPT MODEL CLASS",
   "id": "7c52d59653478bbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = DummyLayerNorm(cfg['emb_dim'])\n",
    "\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.token_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # simple placeholder mean of zero and variance of one\n",
    "        # help stability neural network training and reduce the problem of internal covariate shift\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ],
   "id": "e4fd803862838f33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# STEP 1: TOKENIZATION",
   "id": "8661d12806d2b85f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1 = \"Every effort moves you\"\n",
    "text2 = \"Every day holds a\"\n",
    "batch = []\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ],
   "id": "1ac0e0a20451a006",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# STEP 2: CREATE AN INSTANCE OF DUMMYGPTMODEL",
   "id": "81e4880fd25c7cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "print(model(batch).shape)"
   ],
   "id": "a54f1b9f75c17d49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT ARCHITECTURE PART 2: LAYER NORMALIZATION",
   "id": "b8e9b3e63af4af9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "print(batch_example)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out: torch.Tensor = layer(batch_example)\n",
    "print(out)\n"
   ],
   "id": "574170456d340d66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "out = (out - mean) / torch.sqrt(var)\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "print(out.mean(dim=-1, keepdim=True))"
   ],
   "id": "af934c2e26bc39ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # bessel's correction , the embedding dimension is too large, so the unbiased=True which calculating the var by dividing n-1 instead of n is negligible.\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "fdf8874e315d8bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "out_ln = ln(batch_example)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(out_ln)\n",
    "print(out_ln.mean(dim=-1, keepdim=True))\n",
    "print(out_ln.var(dim=-1, keepdim=True, unbiased=False))\n",
    "# available hardware dictates batch size"
   ],
   "id": "6010b33ade5e8feb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT ARCHITECTURE PART 3: FEEDFORWARD NEURAL NETWORK WITH GELU ACTIVATION",
   "id": "27b004bdac1cb0c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ],
   "id": "edb1484c6eb2da65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']), GELU(),\n",
    "                                    nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "bb7ad36c0fb94c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ],
   "id": "7c47b3614e815a25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT ARCHITECTURE PART 4: SHORTCUT CONNECTIONS",
   "id": "9f0afb92f4901022"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ],
   "id": "ae73f4aad85b38c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "print(sample_input)\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, False)"
   ],
   "id": "924114d7e5e5e5b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ],
   "id": "4d787f9acbfef36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_gradients(model_without_shortcut, sample_input)",
   "id": "7aa06cb5d93989df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ],
   "id": "35412ff1076c52d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT ARCHITECTURE PART 5: CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK",
   "id": "7a1863db9158ec91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ],
   "id": "b9f5e242038d12c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))  # trainable\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        return self.scale * ((x - mean) / torch.sqrt(var)) + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "26f64e8fd50d6d00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg['emb_dim'],\n",
    "            d_out=cfg['emb_dim'],\n",
    "            context_length=cfg['context_length'],\n",
    "            num_heads=cfg['n_heads'],\n",
    "            dropout=cfg['drop_rate'],\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x += shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x += shortcut\n",
    "        return x"
   ],
   "id": "feeb2720bc170317",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(output.shape)"
   ],
   "id": "6f08b4bda7ef2509",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT ARCHITECTURE PART 6: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION",
   "id": "a1c233e1e067c2d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ],
   "id": "1004ed59f502dd41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.token_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg['emb_dim'],\n",
    "            d_out=cfg['emb_dim'],\n",
    "            context_length=cfg['context_length'],\n",
    "            dropout=cfg['drop_rate'],\n",
    "            num_heads=cfg['n_heads'],\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x += shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x += shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # simple placeholder mean of zero and variance of one\n",
    "        # help stability neural network training and reduce the problem of internal covariate shift\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.shift = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        return self.scale * ((x - mean) / torch.sqrt(var + self.eps)) + self.shift"
   ],
   "id": "d383d643efc4412a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1 = \"Every effort moves you\"\n",
    "text2 = \"Every day holds a\"\n",
    "batch = []\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(out.shape)"
   ],
   "id": "76b8e5bcc2c67029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "params = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"{params:,}\")\n",
    "total_size_types = params * 4\n",
    "total_size_mb = total_size_types / (1024 * 1024)\n",
    "print(total_size_mb)\n"
   ],
   "id": "421f0a1d19c247ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT ARCHITECTURE PART 7: GENERATING TEXT GROM OUTPUT TOKENS",
   "id": "9608426410584374"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:11:57.081834Z",
     "start_time": "2025-05-18T06:11:57.077189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "    return idx"
   ],
   "id": "2df92827ff4b6b33",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_text = \"hello, I am\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_text)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)"
   ],
   "id": "20436d8d244839ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()  # bypass disable norm layer, dropout layer\n",
    "out = generate_text_simple(model=model, idx=encoded_tensor, max_new_tokens=6,\n",
    "                           context_size=GPT_CONFIG_124M['context_length'])\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print(tokenizer.decode(out.squeeze(0).tolist()))"
   ],
   "id": "e1a87f4db408845b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,  # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ],
   "id": "f96221f946d55bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:06:52.257949Z",
     "start_time": "2025-05-18T06:06:52.221392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # this unsqueeze(0) just add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat: torch.Tensor = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ],
   "id": "91cf3976f9659c4c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = text_to_token_ids(start_context, tokenizer)\n",
    "predicts = generate_text_simple(model, token_ids, 10, GPT_CONFIG_124M['context_length'])\n",
    "print(token_ids_to_text(predicts, tokenizer))"
   ],
   "id": "2984cfdbdd8514f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CALCULATING THE TEXT GENERATION LOSS: CROSS-ENTROPY AND PERPLEXITY",
   "id": "b2629604f22a8e44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], [40, 1170, 588]])\n",
    "targets = torch.tensor([[3626, 6100, 345], [1170, 588, 11311]])"
   ],
   "id": "e9415f4106a80d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ],
   "id": "a3de5cc8939a9c72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)  # argmax return the indices of the maximum value\n",
    "print(token_ids)"
   ],
   "id": "4c55162e3851c31d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(token_ids_to_text(token_ids[0].flatten(), tokenizer))\n",
    "print(token_ids_to_text(targets[0], tokenizer))"
   ],
   "id": "51731e2604a5f2ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CROSS ENTROPY LOSS",
   "id": "48ecd7f62162dfd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(target_probas_1, target_probas_2)"
   ],
   "id": "6a5b3a565f17b5ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ],
   "id": "52ec537140508cd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_log_probas = log_probas.mean()\n",
    "print(avg_log_probas)"
   ],
   "id": "d9c1b10c00648cc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ],
   "id": "fc76f16f3f44607d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "print(logits_flat)\n",
    "target_flat = targets.flatten(0, 1)\n",
    "print(targets)"
   ],
   "id": "14152d843edef62f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pos_avg_log_probas = nn.functional.cross_entropy(logits_flat, target_flat)",
   "id": "8baf44ca2a5b7d00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PERPLEXITY",
   "id": "6304fcb6543bfff8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# lower perplexity score = better predictions\n",
    "perplexity = torch.exp(pos_avg_log_probas)\n",
    "print(perplexity)\n",
    "# this means model is roughly as uncertain as if it had to choose the next token randomly from about 51492 tokens in the vocabulary"
   ],
   "id": "ce81b4983c40892c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CALCULATING THE TRAINING AND VALIDATION SET LOSSES",
   "id": "33c761aea6f08fff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:10:56.431928Z",
     "start_time": "2025-05-18T06:10:56.429148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ],
   "id": "ec487203ba0d792c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:10:57.457466Z",
     "start_time": "2025-05-18T06:10:57.455093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "id": "ba94cad4328589dd",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:11:11.689702Z",
     "start_time": "2025-05-18T06:11:11.679073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                    stride=GPT_CONFIG_124M['context_length'], drop_last=True,\n",
    "                                    shuffle=True, num_workers=0)\n",
    "\n",
    "val_loader = create_dataloader_v1(val_data, batch_size=2, max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                  stride=GPT_CONFIG_124M['context_length'], drop_last=False,\n",
    "                                  shuffle=False, num_workers=0)"
   ],
   "id": "6f4c9f31a9fa4936",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Sanity check",
   "id": "8bf7f94919de003c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for v in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    # print(x[-1, -10:])\n",
    "    # print(tokenizer.decode(x.flatten(0, 1).tolist()))\n",
    "print(\"-\" * 50)\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    # print(tokenizer.decode(x.flatten(0, 1).tolist()))"
   ],
   "id": "573146565dcce2bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:10:36.119557Z",
     "start_time": "2025-05-18T06:10:36.115948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_loss_batch(input_batch: torch.Tensor, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten(0, 1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ],
   "id": "4f8d56fc79c86e67",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note:\n",
    "Uncommenting the following lines will allow the code to run on Apple silicon chips, if applicable\n",
    "which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "However, the resulting loss values may be slightly different."
   ],
   "id": "c53407768a303d01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")"
   ],
   "id": "96bf517ec6cddce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:11:19.883543Z",
     "start_time": "2025-05-18T06:11:16.226960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking for efficiency because we are not training yet.\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Traning loss:\", train_loss)\n",
    "print(\"Validation loss:\", train_loss)"
   ],
   "id": "392bfb0bd6143556",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning loss: 10.9893217086792\n",
      "Validation loss: 10.9893217086792\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRAINING LOOP FOR THE LLM",
   "id": "23ac5961a4ed6235"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:10:21.997512Z",
     "start_time": "2025-05-18T06:10:21.992726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq,\n",
    "                       eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss: torch.Tensor = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()  # Returns the total number of elements (or tokens) in the input_batch\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch + 1} (Step {global_step:06d}):\"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n"
   ],
   "id": "48936cbe512df7a8",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:10:24.214337Z",
     "start_time": "2025-05-18T06:10:24.211148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ],
   "id": "70298ccc9deda659",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:10:25.306992Z",
     "start_time": "2025-05-18T06:10:25.303554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ],
   "id": "c408349403210846",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:14:26.580194Z",
     "start_time": "2025-05-18T06:12:01.278526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                                                           num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "                                                           start_context=\"Every effort moves you\", tokenizer=tokenizer)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ],
   "id": "aa90de877f33c5cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):Train loss 9.782, Val loss 9.934\n",
      "Ep 1 (Step 000005):Train loss 8.112, Val loss 8.340\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010):Train loss 6.662, Val loss 7.049\n",
      "Ep 2 (Step 000015):Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020):Train loss 5.734, Val loss 6.605\n",
      "Ep 3 (Step 000025):Train loss 5.195, Val loss 6.342\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030):Train loss 4.412, Val loss 6.277\n",
      "Ep 4 (Step 000035):Train loss 4.065, Val loss 6.225\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040):Train loss 3.730, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045):Train loss 2.847, Val loss 6.177\n",
      "Ep 6 (Step 000050):Train loss 2.427, Val loss 6.142\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a and he was no I had the fact, and in the picture.                    \n",
      "Ep 7 (Step 000055):Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060):Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065):Train loss 1.322, Val loss 6.236\n",
      "Ep 8 (Step 000070):Train loss 0.986, Val loss 6.240\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075):Train loss 0.718, Val loss 6.291\n",
      "Ep 9 (Step 000080):Train loss 0.543, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085):Train loss 0.391, Val loss 6.449\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Training completed in 2.42 minutes.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, label=\"Validation loss\", linestyle=\"-.\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # Only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "id": "781c6335c728008f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,  # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ],
   "id": "ae9e82b95a5e8a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MERGE TEMPERATURE SCALING AND TOP-K SAMPLING",
   "id": "ddbe8edc9d1d63f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:17:38.220762Z",
     "start_time": "2025-05-18T06:17:38.216904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "    return idx"
   ],
   "id": "a1d1fe169e17b5be",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T06:17:39.492592Z",
     "start_time": "2025-05-18T06:17:38.925514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(model, text_to_token_ids(\"Every effort moves you\", tokenizer), 15,\n",
    "                     GPT_CONFIG_124M['context_length'], 1.4, 25)\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "88c20df7c9b6b6f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18250,   772]])\n",
      "tensor([[464, 670]])\n",
      "tensor([[  11, 5975]])\n",
      "tensor([[ 286, 5739]])\n",
      "tensor([[764,  13]])\n",
      "tensor([[383, 402]])\n",
      "tensor([[ 271, 1617]])\n",
      "tensor([[261, 286]])\n",
      "tensor([[262, 607]])\n",
      "tensor([[438,  12]])\n",
      "tensor([[12239, 49903]])\n",
      "tensor([[262,   0]])\n",
      "tensor([[ 887, 1375]])\n",
      "tensor([[2045, 3521]])\n",
      "tensor([[470, 523]])\n",
      "Every effort moves youlit evenThe work, surprise of frame .. The Gisrafton of the her---piececolour the! But She looking couldn't so\n"
     ]
    }
   ],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
