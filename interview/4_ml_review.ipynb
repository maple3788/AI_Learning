{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ç¬¬6è®² ä¸€èŠ‚è¯¾æŽŒæ¡Googleæœ€å¼ºå¼ºåŒ–å­¦ä¹ ç®—æ³•",
   "id": "5ce543d6ee633261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.ä»€ä¹ˆæ˜¯on-policy ä»€ä¹ˆæ˜¯off-policy\n",
    "\n",
    "This is one of the most fundamental concepts in Reinforcement Learning. The distinction is all about **what experience the agent learns from**.\n",
    "\n",
    "It comes down to this:\n",
    "* **Target Policy ($\\pi$):** The policy the agent is trying to learn and improve. This is the \"final\" optimal policy it wants to find.\n",
    "* **Behavior Policy ($\\mu$):** The policy the agent *actually uses* to explore the environment and generate experience (i.e., select actions).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. On-Policy Learning\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "In On-Policy learning, the **Target Policy** and the **Behavior Policy** are the **same**.\n",
    "\n",
    "The agent learns by acting with its *current* policy and then updates that *same* policy based on the outcomes.\n",
    "\n",
    "### Analogy\n",
    "\n",
    "**\"Learning on the job.\"**\n",
    "\n",
    "Imagine you are learning to be a chef. You try to cook a dish (your current policy), you see what happens (you burn it), and you update your cooking strategy (update your policy). You are learning directly from your *own* current actions and mistakes.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1.  Use the current policy $\\pi$ to select an action $a$ in state $s$.\n",
    "2.  Observe the reward $r$ and next state $s'$.\n",
    "3.  Use this single piece of experience $(s, a, r, s')$ to update the policy $\\pi$.\n",
    "4.  **Crucially, you must then *discard* this experience.** Why? Because the experience was generated by the \"old\" policy. Once you update $\\pi$ to $\\pi'$, any experience from $\\pi$ is now \"stale\" and no longer relevant.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "* **Pros:**\n",
    "    * Simpler to implement and understand.\n",
    "    * Often more stable and is guaranteed to converge to a (local) optimum.\n",
    "* **Cons:**\n",
    "    * **Extremely sample-inefficient.** You throw away data after every single update. This is a massive problem in real-world applications (like robotics) where collecting experience is slow and expensive.\n",
    "    * It struggles with exploration. The policy has to be \"stochastic\" (e.g., $\\epsilon$-greedy) to try new things, but because it's *also* the target, it's torn between exploring and exploiting.\n",
    "\n",
    "**Example Algorithms:**\n",
    "* **SARSA** (State-Action-Reward-State-Action)\n",
    "* **Policy Gradient** (e.g., REINFORCE, A2C/A3C)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Off-Policy Learning\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "In Off-Policy learning, the **Target Policy** and the **Behavior Policy** are **different**.\n",
    "\n",
    "The agent learns about the optimal policy ($\\pi$) while following a *different* behavior policy ($\\mu$) to explore the environment.\n",
    "\n",
    "### Analogy\n",
    "\n",
    "**\"Learning from a textbook\" or \"Watching a demo.\"**\n",
    "\n",
    "You want to learn how to be a *master* chef (the optimal target policy). You could learn this by reading a book written by a master chef, or by watching videos of *other* people (good and bad) trying to cook (the behavior policy). You are learning about the *best* way to act, regardless of the actions you (or others) are actually taking.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1.  You have two policies:\n",
    "    * **Target Policy $\\pi$:** This is the policy you want to optimize (e.g., a purely greedy policy).\n",
    "    * **Behavior Policy $\\mu$:** This is an exploratory policy you use to collect data (e.g., an $\\epsilon$-greedy policy that takes random actions 10% of the time).\n",
    "2.  Use the behavior policy $\\mu$ to act in the world and collect experience $(s, a, r, s')$.\n",
    "3.  Store this experience in a large **Replay Buffer**.\n",
    "4.  To update the target policy $\\pi$, you randomly sample a *batch* of old experiences from the Replay Buffer.\n",
    "5.  This update corrects for the fact that the data was collected by a different policy (e.g., using Importance Sampling, or the Q-learning $\\max$ operator).\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "* **Pros:**\n",
    "    * **Extremely sample-efficient.** The Replay Buffer allows the agent to reuse a single piece of experience for many, many updates. This is its biggest advantage.\n",
    "    * Better exploration. The behavior policy can be dedicated to exploring (e.g., be very random) while the target policy can focus on being optimal (being greedy).\n",
    "* **Cons:**\n",
    "    * More complex to implement.\n",
    "    * Can be less stable and have higher variance (though modern algorithms have solutions for this).\n",
    "\n",
    "**Example Algorithms:**\n",
    "* **Q-Learning** (and DQN)\n",
    "* **DDPG**, **SAC** (Deep Deterministic Policy Gradient, Soft Actor-Critic)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Q-Learning is Off-Policy (A Common Interview Question)\n",
    "\n",
    "Your course mentions Q-learning. It's the classic example of off-policy. Look at its update rule:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "* **Behavior Policy:** The agent is in state $s$ and uses an $\\epsilon$-greedy policy to choose the *actual* action $a$ it will take.\n",
    "* **Target Policy:** When it updates the Q-value, it looks at the *next* state $s'$ and asks, \"What is the value of the *best possible action* from here?\" This is the **$\\max_{a'}$** part.\n",
    "* The policy it is learning about (the target, \"max\") is a greedy, optimal policy. But the policy it is *using* to get the data (the behavior, \"$\\epsilon$-greedy\") is different. This is the definition of off-policy.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | On-Policy | Off-Policy |\n",
    "| :--- | :--- | :--- |\n",
    "| **Policy Relationship** | Target Policy = Behavior Policy | Target Policy $\\neq$ Behavior Policy |\n",
    "| **Who Generates Data?** | The policy being learned ($\\pi$). | A separate behavior policy ($\\mu$). |\n",
    "| **Use of Old Data?** | **No.** Data is discarded after one use. | **Yes.** Uses a Replay Buffer. |\n",
    "| **Sample Efficiency** | Very Low (Inefficient) | Very High (Efficient) |\n",
    "| **Key Example** | **SARSA**, REINFORCE | **Q-Learning**, DQN |"
   ],
   "id": "f91c9ec607cc9d58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.ä»€ä¹ˆæ ·çš„é—®é¢˜å¯ä»¥è¢«å®šä¹‰ä¸ºå¼ºåŒ–å­¦ä¹ ï¼Ÿ\n",
    "A problem can be defined as a Reinforcement Learning (RL) problem if it can be framed as a **goal-oriented sequential decision-making process under uncertainty**.\n",
    "\n",
    "This means the problem must have a few key components and characteristics:\n",
    "\n",
    "---\n",
    "\n",
    "### The Essential Components\n",
    "\n",
    "A problem is an RL problem if you can clearly identify:\n",
    "\n",
    "1.  **An Agent:** The learner or decision-maker. This is the \"brain\" you are trying to train.\n",
    "    * *Example: The AI player in a chess game.*\n",
    "\n",
    "2.  **An Environment:** The external world that the agent interacts with. It's everything outside the agent.\n",
    "    * *Example: The chess board and the rules of chess.*\n",
    "\n",
    "3.  **A State ($s$):** A snapshot of the environment at a specific moment. It's the information the agent receives to make a decision.\n",
    "    * *Example: The exact position of all pieces on the chess board.*\n",
    "\n",
    "4.  **A set of Actions ($a$):** The possible moves or choices the agent can make in a given state.\n",
    "    * *Example: All legal moves a player can make from the current board state.*\n",
    "\n",
    "5.  **A Reward Signal ($r$):** A scalar feedback value (positive, negative, or zero) that the environment provides after the agent takes an action. This signal is the *only* evaluation of the agent's performance.\n",
    "    * *Example: A +1 for winning the game, -1 for losing, and 0 for every other move.*\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Characteristics\n",
    "\n",
    "Beyond just having these components, the problem *must* have these properties:\n",
    "\n",
    "1.  **Sequential Decision-Making:** The agent must make a *sequence* of decisions over time. The problem is not a single \"one-and-done\" choice.\n",
    "\n",
    "2.  **A Clear Goal (Maximize Cumulative Reward):** The agent's sole objective is to choose actions that maximize the **total accumulated reward** over the long run (the \"return\"), not just the next immediate reward.\n",
    "\n",
    "3.  **Trial-and-Error Learning:** The agent is not given a \"handbook\" of correct answers. It must *discover* the best actions by trying them out (exploration) and then favoring the ones that lead to good outcomes (exploitation).\n",
    "\n",
    "4.  **Delayed Rewards:** This is a hallmark of RL. The reward for a good (or bad) action may not come immediately.\n",
    "    * *Example:* In chess, sacrificing a pawn (a small negative reward, or 0) might be the key to checkmate 20 moves later (a large positive reward). The agent must learn to connect its actions to these delayed consequences.\n",
    "\n",
    "5.  **The Agent's Actions Influence Its Future:** The actions the agent takes must affect the future states it will encounter. If the agent's actions have no bearing on what happens next, it's not a true RL control problem.\n",
    "\n",
    "---\n",
    "\n",
    "### In Summary: RL vs. Other Machine Learning\n",
    "\n",
    "* **Not Supervised Learning:** You are *not* given a dataset of (State, Correct Action) pairs. You are only given (State, Action, Reward) tuples, and the reward is just an *evaluation* (e.g., \"that was bad\"), not an *instruction* (e.g., \"you should have done this instead\").\n",
    "* **Not Unsupervised Learning:** You are *not* just looking for hidden patterns in data. You have a clear, active goal: to maximize a reward signal.\n",
    "\n",
    "**Simple Test:**\n",
    "If you can describe your problem using this loop, it's an RL problem:\n",
    "1.  The **Agent** sees the **State**.\n",
    "2.  The **Agent** takes an **Action**.\n",
    "3.  The **Environment** gives a **Reward** and a new **State**.\n",
    "4.  Repeat... with the goal of getting the most total **Reward** possible.\n",
    "\n",
    "This entire framework is formally known as a **Markov Decision Process (MDP)**."
   ],
   "id": "c5c9604dd5eae69e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.Q-learningå’Œpolicy gradientçš„åŒºåˆ«ï¼Ÿ\n",
    "The fundamental difference is **what they learn** and **how they choose an action**.\n",
    "\n",
    "* **Q-Learning** is **Value-Based**. It learns the *value* of taking an action in a state.\n",
    "* **Policy Gradient** is **Policy-Based**. It learns the *policy* (the probability of taking an action) directly.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Q-Learning (Value-Based)\n",
    "\n",
    "### The Core Idea: Learn a Value Map ðŸ—ºï¸\n",
    "\n",
    "Q-Learning's goal is to learn a function called the **Q-function** (or \"Quality\" function), $Q(s, a)$.\n",
    "\n",
    "This function tells the agent the **expected future reward (the \"value\")** of taking action $a$ in state $s$ and then following the optimal policy forever after.\n",
    "\n",
    "$$Q(s, a) \\approx \\text{Expected total future reward from state } s \\text{ if we take action } a$$\n",
    "\n",
    "### How It Chooses an Action (Implicit Policy)\n",
    "\n",
    "The policy in Q-Learning is **implicit**. It is *derived* from the Q-values.\n",
    "\n",
    "Once the agent has learned an accurate Q-function, the optimal policy is simply to pick the action with the highest Q-value in any given state.\n",
    "\n",
    "**Policy:** $\\pi(s) = \\arg\\max_a Q(s, a)$ (This is a \"greedy\" policy).\n",
    "\n",
    "### Analogy: The Restaurant Critic\n",
    "\n",
    "Think of Q-Learning as a **restaurant critic** who is building a giant guidebook.\n",
    "\n",
    "* The critic's job is to assign a *star rating* ($Q$-value) to every possible *dish* (action) at every *restaurant* (state).\n",
    "* To decide what to eat, you just look at the guidebook for your current restaurant and **pick the dish with the highest rating**.\n",
    "* The critic *doesn't* tell you what to pick; they just give you the values, and your policy is to pick the best one.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Policy Gradient (Policy-Based)\n",
    "\n",
    "### The Core Idea: Learn a Set of Reflexes ðŸŽ¯\n",
    "\n",
    "Policy Gradient (PG) doesn't learn values. It *directly* learns the policy function, $\\pi(a|s)$.\n",
    "\n",
    "This function is a **probability distribution**. It tells the agent, \"Given your current state $s$, here is the *probability* you should take each possible action $a$.\"\n",
    "\n",
    "**Policy:** $\\pi(a|s) = P(\\text{Action } a | \\text{ State } s)$\n",
    "\n",
    "### How It Chooses an Action (Explicit Policy)\n",
    "\n",
    "The policy is **explicit**. The agent's \"brain\" *is* the policy.\n",
    "\n",
    "* To choose an action, the agent just inputs its state $s$ into its policy network and **samples an action** from the resulting probability distribution.\n",
    "* The algorithm then uses \"gradient ascent\" to *increase* the probability of actions that led to high rewards and *decrease* the probability of actions that led to low rewards.\n",
    "\n",
    "### Analogy: The Master Chef\n",
    "\n",
    "Think of Policy Gradient as training a **master chef**.\n",
    "\n",
    "* You don't give the chef a guidebook of ratings. Instead, you train their *instincts* and *reflexes* (the policy).\n",
    "* When you give the chef ingredients (state), they *directly* decide what to do (action) based on their training.\n",
    "* If a dish comes out great (high reward), you \"reinforce\" the instincts that led to it. If it's bad, you \"discourage\" those instincts.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences Summary\n",
    "\n",
    "| Feature | Q-Learning (e.g., DQN) | Policy Gradient (e.g., REINFORCE) |\n",
    "| :--- | :--- | :--- |\n",
    "| **What it Learns** | **Value Function** $Q(s, a)$ (How good is this action?) | **Policy Function** $\\pi(a|s)$ (What is the chance of this action?) |\n",
    "| **Policy** | **Implicit** (Derived from Q-values, e.g., greedy). | **Explicit** (The network *is* the policy). |\n",
    "| **Action Space** | Best for **Discrete** (e.g., Up, Down, Left, Right) because of the $\\max_a$ operation. | Best for **Continuous** (e.g., turn wheel 15.7Â°) because it can output a probability distribution. |\n",
    "| **Sample Efficiency** | **Off-Policy** (can use a Replay Buffer). Very sample efficient. | **On-Policy** (typically). Discards experience after one update. Very sample *in*efficient. |\n",
    "| **Stability** | Can be unstable (e.g., with function approximation), but Replay Buffers help. | More stable, with smoother updates (but can get stuck in local optima). |\n",
    "| **Final Policy** | Deterministic (always picks the best action). | Stochastic (picks actions based on probability). |"
   ],
   "id": "c4c8b1c17375781f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.Reward å’Œvalueçš„åŒºåˆ«ï¼Ÿ\n",
    "This is a critical distinction in reinforcement learning. The simplest way to think about it is:\n",
    "\n",
    "* **Reward** is **immediate** feedback.\n",
    "* **Value** is **long-term**, *predicted* feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Reward (R)\n",
    "\n",
    "A **Reward** is a single, scalar number that the **environment** gives to the **agent** at the *end* of a time step (after the agent takes an action $a$ in state $s$).\n",
    "\n",
    "* **Timescale:** **Short-term / Immediate.**\n",
    "* **Source:** It is *given* by the environment. It is a fundamental part of the problem's definition.\n",
    "* **Purpose:** It defines the \"goal\" for the agent. It tells the agent what is \"good\" or \"bad\" *right now*.\n",
    "* **Analogy:**\n",
    "    * Getting a piece of candy (positive reward).\n",
    "    * Getting an electric shock (negative reward).\n",
    "    * The points you get for eating a pellet in Pac-Man.\n",
    "* **Example (Chess):**\n",
    "    * Action: Make a move.\n",
    "    * **Reward:** You get `+1` *only* on the final move that wins the game. You get `-1` *only* on the final move that loses. You get `0` for every single other move in the entire game.\n",
    "\n",
    "### 2. Value (V or Q)\n",
    "\n",
    "A **Value** (or \"Value Function\") is a **prediction** of the **total future reward** the **agent** *expects* to receive, starting from a given state.\n",
    "\n",
    "* **Timescale:** **Long-term / Predictive.**\n",
    "* **Source:** It is *learned* and *estimated* by the agent. It is the *solution* to the problem, not part of the problem itself.\n",
    "* **Purpose:** It tells the agent what is \"good\" or \"bad\" *in the long run*. This is what the agent actually uses to make decisions.\n",
    "* **Analogy:**\n",
    "    * Your current bank account balance + all your *expected* future income (a long-term value).\n",
    "    * Your overall \"health\" (a long-term state).\n",
    "* **Example (Chess):**\n",
    "    * The **Reward** for 99% of moves is `0`. This is not useful for deciding which move is good.\n",
    "    * The **Value** of a board state (e.g., $V(s)$) is the *expected total future reward* from that state. In chess, this is equivalent to the **probability of winning** from that state.\n",
    "    * A move that traps the opponent's queen (Reward = `0`) leads to a *new state* with a much higher **Value** (higher win probability).\n",
    "\n",
    "---\n",
    "\n",
    "### The Relationship: How They Work Together\n",
    "\n",
    "The agent's entire job is to learn the **Value Function** so it can make good decisions.\n",
    "\n",
    "It learns the value function by using the **Rewards** it receives.\n",
    "\n",
    "The **Value** of a state is the *sum of all future rewards* the agent expects to get. A high-value state is one that will lead to many high rewards in the future.\n",
    "\n",
    "**Value is the prediction; Reward is the data used to make that prediction.**\n",
    "\n",
    "The agent uses the immediate, short-term **Rewards** to update its estimate of the long-term **Value**. This is the core of Reinforcement Learning, defined by the Bellman equation:\n",
    "\n",
    "> The **Value** of a state today = (The **Reward** I get immediately) + (The discounted **Value** of the state I end up in tomorrow)\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Feature | Reward (R) | Value (V or Q) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Timescale** | **Immediate** (Short-term) | **Predictive** (Long-term) |\n",
    "| **Source** | Provided by the **Environment** | Learned & estimated by the **Agent** |\n",
    "| **What it answers** | \"Was that action good *right now*?\" | \"Is this state (or state-action) *ultimately* good?\" |\n",
    "| **Role in RL** | The *objective signal* to be maximized. | The *prediction* used to make optimal decisions. |"
   ],
   "id": "1571b7b22a49d8e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.ä¸ºä»€ä¹ˆæŽ¨èå¯ä»¥çœ‹æˆæ˜¯å¼ºåŒ–å­¦ä¹ çš„é—®é¢˜ï¼Ÿ\n",
    "Viewing recommendation as a reinforcement learning (RL) problem is a powerful, modern approach. It's a natural fit because a user's interaction with a platform is not a single event, but a **sequential decision-making process**.\n",
    "\n",
    "The core idea is to shift the objective from **\"predicting the next click\"** (a static, supervised learning problem) to **\"learning a policy that maximizes long-term user engagement\"** (a dynamic, RL problem).\n",
    "\n",
    "Here is how the problem is framed in RL terms, followed by *why* this is so much more powerful.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Mapping Recommendation to the RL Framework\n",
    "\n",
    "First, we must be able to define the problem using the core components of RL (Agent, Environment, State, Action, Reward).\n",
    "\n",
    "* **Agent:** The recommendation system (the algorithm or \"policy\") that is making the decisions.\n",
    "* **Environment:** The user (and the platform, e.g., the app or website) that the agent interacts with.\n",
    "* **State ($s$):** A representation of the user and their context *at this moment*. This includes:\n",
    "    * User's historical data (watch history, purchase history, past ratings).\n",
    "    * Current context (time of day, device, user's current mood or \"session\").\n",
    "    * The last few items the user has seen or interacted with.\n",
    "* **Action ($a$):** The decision the agent makes. This is the **item (or list of items) to recommend** to the user *right now*.\n",
    "    * *Example: Showing a specific video, product, or news article.*\n",
    "* **Reward ($r$):** The feedback from the user, which measures the \"goodness\" of the action. This can be:\n",
    "    * **Positive Reward:** Click, long watch time, \"like\", purchase, add to cart.\n",
    "    * **Zero or Negative Reward:** Skip, scroll past, \"dislike\", exit the app.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why This is More Powerful Than Traditional Methods\n",
    "\n",
    "Simply mapping the components isn't enough. The *reason* RL is a better fit is because it captures dynamics that traditional methods (like collaborative filtering or content-based filtering) ignore.\n",
    "\n",
    "#### 1. It Optimizes for Long-Term Rewards (Engagement)\n",
    "\n",
    "This is the **most important reason**.\n",
    "\n",
    "* **Traditional (Supervised) Problem:** \"Predict the Click-Through-Rate (CTR).\" This optimizes for an **immediate, short-term reward** (the click). This can lead to \"clickbait\" recommendations. The system learns to show items that are \"clickable\" but not necessarily \"satisfying.\"\n",
    "* **RL Problem:** \"Maximize the *cumulative* reward over the user's entire session (or lifetime).\" This is the **long-term reward**.\n",
    "* **Example:**\n",
    "    * A \"clickbait\" video might get a +1 (for the click) but then an immediate \"exit,\" leading to a total session reward of `+1`.\n",
    "    * A documentary recommendation might be *skipped* (reward `0`), but the *next* recommendation (another documentary) is clicked and watched for an hour (reward `+60`).\n",
    "    * The RL agent can learn to sacrifice the immediate reward (risk the skip) to build a user state that leads to a much higher total reward (long-term engagement).\n",
    "\n",
    "#### 2. It's a Sequential Problem (Actions Affect States)\n",
    "\n",
    "In a recommendation setting, the agent's actions *change* the environment (the user's state).\n",
    "\n",
    "1.  **State 1:** User has watched comedy.\n",
    "2.  **Action 1:** Agent recommends a specific *action* movie.\n",
    "3.  User clicks and watches it.\n",
    "4.  **State 2:** User has now watched comedy *and* an action movie.\n",
    "\n",
    "The user's \"state\" is now different, directly because of the agent's action. The user's interests may have temporarily (or permanently) shifted. Traditional models treat every recommendation as a separate, independent prediction. RL is designed to handle this exact loop, where `Action` -> `New State` -> `New Action`.\n",
    "\n",
    "#### 3. It Naturally Handles the Exploration vs. Exploitation Trade-off\n",
    "\n",
    "This is a classic RL problem that is a perfect fit for recommendations.\n",
    "\n",
    "* **Exploitation:** Show the user what you *know* they like. If they always watch \"Marvel,\" show them another \"Marvel\" movie. This is safe and gives a predictable, positive reward.\n",
    "* **Exploration:** Show the user something *new* and *different* (e.g., an indie film). This is riskyâ€”the user might skip it (zero reward). But it's the *only* way to **discover new user interests** and gather new data. If the user loves it, you've \"unlocked\" a new, high-reward area for future recommendations.\n",
    "\n",
    "If a system *only* exploits, it creates a \"filter bubble\" and the user gets bored. If it *only* explores, it gives too many bad recommendations. RL algorithms (like Q-learning, Policy Gradients) are explicitly designed to find the optimal balance between these two.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In short, framing recommendation as an RL problem shifts the goal from **\"predicting a click\"** (Supervised Learning) to **\"learning a policy that creates a satisfying and engaging long-term user experience\"** (Reinforcement Learning)."
   ],
   "id": "c83c368dcc3ffe40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ç¬¬7è®² å¦‚ä½•åº”å¯¹ç™¾å˜machine learning ç³»ç»Ÿè®¾è®¡å’Œ A/B Testï¼Œä»¥Two-Sigmaçš„æ—¶é—´åºåˆ—é¢„æµ‹ä¸ºä¾‹",
   "id": "ae278ab1bf1d75b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.æ—¶é—´åºåˆ—æ•°æ®æ€Žä¹ˆåšé¢„å¤„ç†\n",
    "Here is a comprehensive breakdown of how to preprocess time-series data, structured for a machine learning system design discussion.\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing for Time-Series Data\n",
    "\n",
    "Preprocessing time-series data is fundamentally different from preprocessing i.i.d. (independent and identically distributed) data, like images or user profiles. The central challenge is that **order matters**. Every preprocessing step must respect the temporal sequence to prevent data leakage and to correctly model temporal dependencies.\n",
    "\n",
    "Here is a 7-step process for robustly preprocessing time-series data.\n",
    "\n",
    "### 1. Handling the Time Index (The Foundation)\n",
    "\n",
    "This is the most critical first step. Your model needs to understand time.\n",
    "\n",
    "* **Parse Dates:** Convert your timestamp column (e.g., a string) into a `datetime` object.\n",
    "* **Set as Index:** Set this `datetime` column as the DataFrame index. This makes all subsequent temporal operations (like resampling) much easier.\n",
    "* **Check Uniformity:** Verify that the time intervals are regular (e.g., every 1 minute, every 1 hour).\n",
    "* **Resampling (if irregular):** If data is irregular (e.g., stock trades, user events), you must resample it to a fixed frequency.\n",
    "    * **Downsampling:** (e.g., 1-minute data -> 1-hour data). You must aggregate. Common methods:\n",
    "        * `mean()`: Average value over the hour.\n",
    "        * `sum()`: Total value over the hour (e.g., sales).\n",
    "        * `last()`: The last recorded value.\n",
    "        * **OHLC:** For finance (like Two-Sigma), you'd take the `Open`, `High`, `Low`, and `Close` values within that interval.\n",
    "    * **Upsampling:** (e.g., 1-hour data -> 1-minute data). This creates gaps, which leads to the next step.\n",
    "\n",
    "### 2. Handling Missing Values (Imputation)\n",
    "\n",
    "You cannot simply drop a row, as this would break the time sequence. You also cannot fill with the *global* mean, as that leaks future information and ignores the current trend.\n",
    "\n",
    "* **Forward Fill (`ffill`):** Fills a `NaN` with the *last known value*. This is the most common and \"safest\" method. It assumes the state hasn't changed. This is a good default.\n",
    "* **Backward Fill (`bfill`):** Fills a `NaN` with the *next known value*. This can cause lookahead bias and is generally less common, but useful in some contexts.\n",
    "* **Linear Interpolation:** Draws a straight line between the two known points surrounding the gap. This is a good choice if the data is generally smooth and non-volatile.\n",
    "* **Rolling Mean Imputation:** Uses the mean of the last $k$ data points (e.g., the last 7 days) to fill the missing value. This is more adaptive than a global mean.\n",
    "* **Seasonal Imputation:** For highly seasonal data (e.g., retail sales), you might fill a missing Monday's value with the average of the previous 4 Mondays.\n",
    "\n",
    "### 3. Feature Engineering (Creating Predictors)\n",
    "\n",
    "This is where you create the signals for your model. The raw value $y_t$ is rarely enough.\n",
    "\n",
    "* **Lag Features:** This is the most important feature. The value at a previous time step $t-k$ is used to predict the value at time $t$. You can create multiple lags (e.g., $t-1$, $t-2$, $t-12$ for monthly data).\n",
    "* **Rolling Window Features:** These capture recent trends and volatility.\n",
    "    * **Rolling Mean:** The average of the last $k$ periods (e.g., 7-day moving average). This smooths out noise.\n",
    "    * **Rolling Std. Deviation:** The standard deviation of the last $k$ periods. This is a key measure of **volatility**.\n",
    "    * Other aggregates like `min`, `max`, `sum` over the window.\n",
    "* **Date/Time Features:** Extract information from the `datetime` index itself. This is critical for capturing cycles.\n",
    "    * Hour of day\n",
    "    * Day of week\n",
    "    * Month of year\n",
    "    * Quarter\n",
    "    * `is_weekend` (binary flag)\n",
    "    * `is_holiday` (binary flag)\n",
    "* **Interaction Features:** Combine features, e.g., `day_of_week * hour_of_day`, to capture complex patterns (like \"lunch rush on a Friday\").\n",
    "\n",
    "### 4. Making the Data Stationary\n",
    "\n",
    "**Stationarity** means the statistical properties of the series (like mean and variance) do not change over time.\n",
    "* **Why?** Classical models (like ARIMA) *require* it. Even if not strictly required, many models (including NNs) perform better on a stationary series because it's easier to predict.\n",
    "* **How to achieve it:**\n",
    "    * **Differencing (Detrending):** The most common method. Instead of predicting $y_t$, you predict $y'_t = y_t - y_{t-1}$. This removes a linear trend.\n",
    "    * **Seasonal Differencing:** $y'_t = y_t - y_{t-12}$ (for monthly data with a yearly cycle).\n",
    "    * **Log Transform:** Use $\\log(y_t)$. This is very effective if the variance *grows* with the mean (e.g., exponential growth). It stabilizes the variance.\n",
    "\n",
    "### 5. Scaling and Normalization\n",
    "\n",
    "Many models (like Neural Networks and SVMs) require features to be on a similar scale.\n",
    "\n",
    "* **StandardScaler:** (Z-score normalization). Subtracts the mean and divides by the standard deviation.\n",
    "* **MinMaxScaler:** Scales the data to be between a fixed range (e.g., [0, 1]).\n",
    "\n",
    "**CRITICAL PITFALL:** You **must not** fit your scaler on the entire dataset. This would cause **data leakage**, as you'd be using the mean and std. dev. from the future (test set) to scale the past (train set).\n",
    "\n",
    "**Correct Procedure:**\n",
    "1.  Split your data into train and test sets (see next step).\n",
    "2.  Fit the scaler **only** on the **training data** (`scaler.fit(train_data)`).\n",
    "3.  Use that *same* fitted scaler to `transform` both the training and test data.\n",
    "\n",
    "### 6. Handling Outliers and Noise\n",
    "\n",
    "* **Smoothing:** Applying a rolling mean (as a feature or a preprocessing step) can help reduce the impact of random noise.\n",
    "* **Clipping/Capping:** You can cap values at a certain percentile (e.g., 1st and 99th) to remove extreme, unphysical values.\n",
    "* **System Design Discussion:** In finance (like Two-Sigma), a market crash is **not an outlier**; it's a critical event to be modeled. In sensor data, a spike to 1,000,000 is clearly an error. You must use domain knowledge to distinguish between *errors* and *rare events*.\n",
    "\n",
    "### 7. Splitting the Data (Train/Validation/Test)\n",
    "\n",
    "This is the final and most important preprocessing step.\n",
    "\n",
    "**CRITICAL PITFALL:** You **cannot** use `sklearn.model_selection.train_test_split`. It shuffles the data randomly, which completely destroys the temporal order and makes your model useless.\n",
    "\n",
    "**Correct Procedure (Time-Based Split):**\n",
    "Your validation *must* simulate the future.\n",
    "\n",
    "* **Simple Split:** Train on all data from 2010-2018. Validate on 2019. Test on 2020.\n",
    "* **Time-Series Cross-Validation (Backtesting):** This is the gold standard, especially in finance.\n",
    "    * **Expanding Window:**\n",
    "        * Fold 1: Train on [Year 1], Test on [Year 2]\n",
    "        * Fold 2: Train on [Year 1, 2], Test on [Year 3]\n",
    "        * Fold 3: Train on [Year 1, 2, 3], Test on [Year 4]\n",
    "    * **Sliding Window:**\n",
    "        * Fold 1: Train on [Year 1, 2], Test on [Year 3]\n",
    "        * Fold 2: Train on [Year 2, 3], Test on [Year 4]\n",
    "        * Fold 3: Train on [Year 3, 4], Test on [Year 5]"
   ],
   "id": "ac9eaaf4d08370c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
