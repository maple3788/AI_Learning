{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "outputs = (inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias)\n",
    "\n",
    "print(outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "outputs = (inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + inputs[2] * weights[2] + bias)\n",
    "\n",
    "print(outputs)"
   ],
   "id": "a398e0f43d660442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "bias = [2, 3, 0.5]\n",
    "\n",
    "outputs = [\n",
    "    inputs[0] * weights[0][0] +\n",
    "    inputs[1] * weights[0][1] +\n",
    "    inputs[2] * weights[0][2] +\n",
    "    inputs[3] * weights[0][3] +\n",
    "    bias[0],\n",
    "    inputs[0] * weights[1][0] +\n",
    "    inputs[1] * weights[1][1] +\n",
    "    inputs[2] * weights[1][2] +\n",
    "    inputs[3] * weights[1][3] +\n",
    "    bias[1],\n",
    "    inputs[0] * weights[2][0] +\n",
    "    inputs[1] * weights[2][1] +\n",
    "    inputs[2] * weights[2][2] +\n",
    "    inputs[3] * weights[2][3] +\n",
    "    bias[2],\n",
    "]\n",
    "print(outputs)\n",
    "outputs = []\n",
    "for neuron, bias in zip(weights, bias):\n",
    "    tmp_res = 0\n",
    "    for weight, input in zip(neuron, inputs):\n",
    "        tmp_res += weight * input\n",
    "    outputs.append(tmp_res + bias)\n",
    "print(outputs)"
   ],
   "id": "3c699671191a1411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "outputs = np.dot(inputs, weights) + bias\n",
    "print(outputs)"
   ],
   "id": "39075a2a1898acef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "bias = [2, 3, 0.5]\n",
    "\n",
    "outputs = np.dot(weights, inputs) + bias\n",
    "print(outputs)\n",
    "\n",
    "test1 = [1, 2, 3]\n",
    "test2 = [2, 3, 4]\n",
    "print(test1 + test2)\n",
    "print(np.transpose(test1) + test2)"
   ],
   "id": "251512da8d3cb79f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.5, 3, 3.5, 4.5],\n",
    "          [4.5, 5, 5.5, 6.5]]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "bias = [[2, 3, 0.5] for _ in range(3)]\n",
    "print(np.transpose(weights))\n",
    "outputs = np.dot(inputs, np.transpose(weights))\n",
    "outputs = np.dot(inputs, np.transpose(weights)) + bias\n",
    "print(outputs)\n"
   ],
   "id": "ea9ce8ea6f29d0ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ],
   "id": "61e0c67416a41f0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg')\n",
    "plt.show()"
   ],
   "id": "83ba80ee7149ee3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "A = np.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "print(A)\n",
    "print(A - np.max(A, axis=1, keepdims=True))\n"
   ],
   "id": "b481df53fc5c6a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2, 5, -1, 2],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "print(exp)\n",
    "\n",
    "probability = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "print(probability)"
   ],
   "id": "1367cb233705fc5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, inputs, neurons):\n",
    "        self.layer = 0.01 * np.random.randn(inputs, neurons)  # weight size (inputs, neurons)\n",
    "        self.bias = np.zeros((1, neurons))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.layer) + self.bias\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "class SoftMax:\n",
    "    def forward(self, x):\n",
    "        exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)"
   ],
   "id": "a71a704b61bab829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "    def forward(self, y_pred, y_ture):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clip = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            loss = y_pred_clip[range(len(y_pred_clip)), y_true]\n",
    "        else:\n",
    "            loss = np.sum(y_pred_clip * y_true, keepdims=True, axis=1)\n",
    "        loss_res = -np.log(loss)\n",
    "        return loss_res\n",
    "\n",
    "\n",
    "lc = Loss_CategoricalCrossEntropy()\n",
    "print(lc.calculate(np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.08, 0.9, 0.02]]),\n",
    "                   np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0]])))\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1], [0.1, 0.5, 0.4], [0.08, 0.9, 0.02]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print(accuracy)"
   ],
   "id": "f0c0c3909b1a0337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target_output = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "\n",
    "for iteration in range(200):\n",
    "    linear_output = np.dot(inputs, weights) + bias\n",
    "    outputs = relu(linear_output)\n",
    "    loss = (outputs - target_output) ** 2\n",
    "\n",
    "    dloss_doutput = 2 * (outputs - target_output)\n",
    "    drelu_loss = relu_derivative(outputs)\n",
    "    weight_loss = inputs\n",
    "    bias_loss = bias\n",
    "\n",
    "    change_weight = dloss_doutput * drelu_loss * weight_loss\n",
    "    change_bias = dloss_doutput * drelu_loss * 1\n",
    "\n",
    "    weights -= learning_rate * change_weight\n",
    "    bias -= learning_rate * change_bias\n",
    "\n",
    "    print(iteration, loss)\n",
    "print(weights, bias)"
   ],
   "id": "3378eaea98c15874",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = np.array([1, 2, 3, 4])\n",
    "\n",
    "weights = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "bias = np.array([0.1, 0.2, 0.3])\n",
    "target_output = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "\n",
    "for iteration in range(181):\n",
    "    linear_output = np.dot(inputs, weights.T) + bias\n",
    "    outputs = relu(linear_output)\n",
    "    loss = (np.sum(outputs) - target_output) ** 2\n",
    "\n",
    "    dloss_doutput = 2 * (np.sum(outputs) - target_output)\n",
    "    drelu_loss = relu_derivative(outputs)\n",
    "    weight_loss = inputs\n",
    "    bias_loss = bias\n",
    "    change_weight = np.outer(dloss_doutput, weight_loss)\n",
    "    change_bias = dloss_doutput * drelu_loss * 1\n",
    "\n",
    "    weights -= learning_rate * change_weight\n",
    "    bias -= learning_rate * change_bias\n",
    "\n",
    "    print(iteration, loss)\n",
    "print(weights, bias)"
   ],
   "id": "145aae742784fff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([\n",
    "    [1., 1., 1.],\n",
    "    [2., 2., 2.],\n",
    "    [3., 3., 3.]\n",
    "])\n",
    "inputs = np.array([\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2., 5., -1., 2],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)\n",
    "weights = np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]).T\n",
    "\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)"
   ],
   "id": "a649d079198a9483",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clip = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            loss = y_pred_clip[range(len(y_pred_clip)), y_true]\n",
    "        else:\n",
    "            loss = np.sum(y_pred_clip * y_true, keepdims=True, axis=1)\n",
    "        loss_res = -np.log(loss)\n",
    "        return loss_res\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs /= samples"
   ],
   "id": "5af71dbd17f4a9e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = 4\n",
    "y_true = [1, 2, 3]\n",
    "y_true = np.eye(labels)[y_true]\n",
    "print(y_true)"
   ],
   "id": "819768db63f04b89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def forward(self, layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            weights_update = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weights_update\n",
    "            bias_update = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_update\n",
    "        else:\n",
    "            weights_update = - self.current_learning_rate * layer.dweights\n",
    "            bias_update = - self.current_learning_rate * layer.dbiases\n",
    "        layer.weights += weights_update\n",
    "        layer.biases += bias_update\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "d577806f6ec4a0dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Optimizer_Adagrad:\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def forward(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "d62f5516f6d89e9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def forward(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases ** 2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (\n",
    "                np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (\n",
    "                np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "4dea871e28a0e643"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
